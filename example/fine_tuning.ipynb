{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine tuning the pretrained model\n",
    "In this notebook, we will fine-tune the pretrained model on a custom dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import pickle\n",
    "import jax\n",
    "from tqdm import tqdm\n",
    "from flax.training import train_state\n",
    "import optax\n",
    "import numpy as np\n",
    "from ase.io import Trajectory\n",
    "from typing import Any\n",
    "from reaxnet.egnn.nequip import NequIPEnergyModel\n",
    "from reaxnet.egnn.data import AtomicNumberTable, load_from_atomic_list, graph_from_configuration\n",
    "from reaxnet.egnn.dataloader import GraphDataLoader\n",
    "from reaxnet.egnn.loss import WeightedLossFunction, EvaluationLossFunction\n",
    "from reaxnet.egnn.compute import compute_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the dataset\n",
    "The demo dataset only contains the equivariant graph neural network potential references because it has been aligned with the long-range interactions:\n",
    "\n",
    "$E_{\\rm ref} = E_{\\rm DFT} - E_{\\rm long-range}$\n",
    "\n",
    "$F_{\\rm ref} = F_{\\rm DFT} - F_{\\rm long-range}$\n",
    "\n",
    "$S_{\\rm ref} = S_{\\rm DFT} - S_{\\rm long-range}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_split(num, ratio):\n",
    "    indices = np.random.permutation(num)\n",
    "    split = int(num * ratio)\n",
    "    train_idx = indices[split:].tolist()\n",
    "    val_idx = indices[:split].tolist()\n",
    "    return train_idx, val_idx\n",
    "traj = Trajectory('demo.traj')\n",
    "val_ratio = 0.2\n",
    "train_idx, val_idx = random_split(len(traj), val_ratio)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = '../pretrained/'\n",
    "with open(model_path+'model_config.yaml', 'r') as f:\n",
    "    model_dict = yaml.safe_load(f)\n",
    "with open(model_path+'params.pickle', 'rb') as f:\n",
    "    params = pickle.load(f)\n",
    "ztable = AtomicNumberTable.from_dict(model_path+'mapping.yaml')\n",
    "model = NequIPEnergyModel(**model_dict) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading configs from trajectory: 100%|██████████| 500/500 [00:00<00:00, 779.28it/s]\n"
     ]
    }
   ],
   "source": [
    "configs = load_from_atomic_list(traj, model_dict['r_max'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiplier = 2.5\n",
    "batch_size = 32\n",
    "train_loader = GraphDataLoader([graph_from_configuration(configs[i], z_table=ztable) for i in train_idx],\n",
    "                               n_node=int(model_dict['n_neighbors'] * batch_size * multiplier),\n",
    "                               n_edge=int(model_dict['n_neighbors'] * batch_size * multiplier),\n",
    "                               n_graph=batch_size,\n",
    "                               shuffle=False)\n",
    "val_loader = GraphDataLoader([graph_from_configuration(configs[i], z_table=ztable) for i in val_idx],\n",
    "                                n_node=int(model_dict['n_neighbors'] * batch_size * multiplier),\n",
    "                                n_edge=int(model_dict['n_neighbors'] * batch_size * multiplier),\n",
    "                                n_graph=batch_size,\n",
    "                                shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Freezing the model parameters\n",
    "Here, we will freeze the model parameters to only train the last two linear layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_params(params, trainable_layers=None):\n",
    "    if trainable_layers is None:\n",
    "        return {}, params\n",
    "    frozen_params = {}\n",
    "    trainable_params = {}\n",
    "    for top_key, top_value in params.items():\n",
    "        if isinstance(top_value, dict):\n",
    "            frozen_sub = {}\n",
    "            trainable_sub = {}\n",
    "            \n",
    "            for key, value in top_value.items():\n",
    "                if key in trainable_layers:\n",
    "                    trainable_sub[key] = value\n",
    "                else:\n",
    "                    frozen_sub[key] = value\n",
    "            \n",
    "            if frozen_sub:\n",
    "                frozen_params[top_key] = frozen_sub\n",
    "            if trainable_sub:\n",
    "                trainable_params[top_key] = trainable_sub\n",
    "        else:\n",
    "            if top_key in trainable_layers:\n",
    "                trainable_params[top_key] = top_value\n",
    "            else:\n",
    "                frozen_params[top_key] = top_value\n",
    "    \n",
    "    return frozen_params, trainable_params\n",
    "\n",
    "trainable_layers = ['Linear_1', 'Linear_2']\n",
    "frozen_params, trainable_params = split_params(params, trainable_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All layers: ['BesselEmbedding_0', 'Linear_0', 'Linear_1', 'Linear_2', 'NequIPConvolution_0', 'NequIPConvolution_1', 'NequIPConvolution_2', 'NequIPConvolution_3', 'NequIPConvolution_4', 'NequIPConvolution_5']\n",
      "Fine-tuning layers: ['Linear_1', 'Linear_2']\n",
      "Frozen layers: ['BesselEmbedding_0', 'Linear_0', 'NequIPConvolution_0', 'NequIPConvolution_1', 'NequIPConvolution_2', 'NequIPConvolution_3', 'NequIPConvolution_4', 'NequIPConvolution_5']\n"
     ]
    }
   ],
   "source": [
    "if 'params' in params:\n",
    "    all_layers = list(params['params'].keys())\n",
    "    frozen_layers = [layer for layer in all_layers if layer not in trainable_layers]\n",
    "    print(f\"All layers: {all_layers}\")\n",
    "    print(f\"Fine-tuning layers: {trainable_layers}\")\n",
    "    print(f\"Frozen layers: {frozen_layers}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the training state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FineTuneTrainState(train_state.TrainState):\n",
    "    frozen_params: Any\n",
    "    loss_scale: float\n",
    "\n",
    "def create_finetune_state(\n",
    "    pretrained_params,\n",
    "    trainable_layers,\n",
    "    model,\n",
    "    learning_rate=1e-3,\n",
    "    weight_decay=0\n",
    "):\n",
    "    frozen_params, trainable_params = split_params(pretrained_params, trainable_layers)\n",
    "    \n",
    "    optimizer = optax.adamw(learning_rate=learning_rate, weight_decay=weight_decay)\n",
    "    return FineTuneTrainState.create(\n",
    "        apply_fn=model.apply,\n",
    "        params=trainable_params,\n",
    "        tx=optimizer,\n",
    "        frozen_params=frozen_params,\n",
    "        loss_scale=1.0,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the optimizer and loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merged_forward(state, batch):\n",
    "    merged_params = {}\n",
    "    for key in set(list(state.frozen_params.keys()) + list(state.params.keys())):\n",
    "        merged_params[key] = {}\n",
    "        if key in state.frozen_params:\n",
    "            if isinstance(state.frozen_params[key], dict):\n",
    "                for sub_key, sub_value in state.frozen_params[key].items():\n",
    "                    merged_params[key][sub_key] = sub_value\n",
    "            else:\n",
    "                merged_params[key] = state.frozen_params[key]\n",
    "        if key in state.params:\n",
    "            if isinstance(state.params[key], dict):\n",
    "                for sub_key, sub_value in state.params[key].items():\n",
    "                    merged_params[key][sub_key] = sub_value\n",
    "            else:\n",
    "                merged_params[key] = state.params[key]\n",
    "    return compute_fn(model=model, params=merged_params, graph=batch)\n",
    "\n",
    "energy_weight = 1.0\n",
    "forces_weight = 1.0\n",
    "stress_weight = 1.0\n",
    "_loss_fn = WeightedLossFunction(energy_weight=energy_weight, forces_weight=forces_weight, stress_weight=stress_weight)\n",
    "_evaluate_fn = EvaluationLossFunction(energy_weight=energy_weight, forces_weight=forces_weight, stress_weight=stress_weight)\n",
    "\n",
    "@jax.jit\n",
    "def compute_loss(state, batch):\n",
    "    return _loss_fn(batch, merged_forward(state, batch))\n",
    "\n",
    "@jax.jit\n",
    "def evaluate_fn(state, batch):\n",
    "    return _evaluate_fn(batch, merged_forward(state, batch))\n",
    "\n",
    "@jax.jit\n",
    "def train_step(state, batch):\n",
    "    def loss_fn(params):\n",
    "        state_with_params = state.replace(params=params)\n",
    "        loss = compute_loss(\n",
    "            state_with_params, batch\n",
    "        )\n",
    "        return loss\n",
    "    loss, grads = jax.value_and_grad(loss_fn, has_aux=False)(state.params)\n",
    "    state = state.apply_gradients(grads=grads)\n",
    "    return state, loss\n",
    "\n",
    "@jax.jit\n",
    "def evaluate(state, batch):\n",
    "    results = evaluate_fn(state, batch)\n",
    "    total_loss = results[0]\n",
    "    rmse_e = results[1]\n",
    "    rmse_f = results[2]\n",
    "    rmse_s = results[3]\n",
    "    mae_e = results[4]\n",
    "    mae_f = results[5]\n",
    "    mae_s = results[6]\n",
    "    return total_loss, rmse_e, rmse_f, rmse_s, mae_e, mae_f, mae_s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finetune(\n",
    "    state,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    epochs=100,\n",
    "):\n",
    "    best_val_loss = float('inf')\n",
    "    best_state = state\n",
    "    \n",
    "    epochs_bar = tqdm(range(epochs), desc=\"Training\")\n",
    "    \n",
    "    for epoch in epochs_bar:\n",
    "        epoch_train_losses = []\n",
    "        for batch in train_loader:\n",
    "            state, loss = train_step(state, batch)\n",
    "            epoch_train_losses.append(loss)\n",
    "        \n",
    "        avg_train_loss = np.mean(epoch_train_losses)\n",
    "        \n",
    "        epoch_val_losses = []\n",
    "        epoch_val_energy_mae = []\n",
    "        epoch_val_force_mae = []\n",
    "        epoch_val_stress_mae = []\n",
    "        for batch in val_loader:\n",
    "            val_loss, rmse_e, rmse_f, rmse_s, mae_e, mae_f, mae_s = evaluate(state, batch)\n",
    "            epoch_val_losses.append(val_loss)\n",
    "            epoch_val_energy_mae.append(mae_e)\n",
    "            epoch_val_force_mae.append(mae_f)\n",
    "            epoch_val_stress_mae.append(mae_s) \n",
    "        avg_val_loss = np.mean(epoch_val_losses)\n",
    "        avg_val_energy_loss = np.mean(epoch_val_energy_mae)\n",
    "        avg_val_force_loss = np.mean(epoch_val_force_mae)\n",
    "        avg_val_stress_loss = np.mean(epoch_val_stress_mae)\n",
    "\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            best_state = state\n",
    "        \n",
    "        print(\"{:>15} {:>15} {:>15} {:>15} {:>15} {:>15}\".format(\n",
    "            \"Epoch\", \"Train Loss\", \"Val Loss\", \"Energy MAE\", \"Force MAE\", \"Stress MAE\"))\n",
    "        print(\"{:>15} {:>15.6f} {:>15.6f} {:>15.6f} {:>15.6f} {:>15.6f}\".format(\n",
    "            epoch, avg_train_loss, avg_val_loss, avg_val_energy_loss, avg_val_force_loss, avg_val_stress_loss))\n",
    "\n",
    "            \n",
    "    return best_state\n",
    "        \n",
    "finetune_state = create_finetune_state(\n",
    "    params,\n",
    "    trainable_layers,\n",
    "    model,\n",
    "    learning_rate=1e-4,\n",
    "    weight_decay=0,\n",
    ")\n",
    "best_state = finetune(\n",
    "    finetune_state,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    epochs=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_full_params(state):\n",
    "    full_params = {}\n",
    "    for key in set(list(state.frozen_params.keys()) + list(state.params.keys())):\n",
    "        full_params[key] = {}\n",
    "        if key in state.frozen_params:\n",
    "            if isinstance(state.frozen_params[key], dict):\n",
    "                for sub_key, sub_value in state.frozen_params[key].items():\n",
    "                    full_params[key][sub_key] = sub_value\n",
    "            else:\n",
    "                full_params[key] = state.frozen_params[key]\n",
    "        if key in state.params:\n",
    "            if isinstance(state.params[key], dict):\n",
    "                for sub_key, sub_value in state.params[key].items():\n",
    "                    full_params[key][sub_key] = sub_value\n",
    "            else:\n",
    "                full_params[key] = state.params[key]\n",
    "    return full_params\n",
    "\n",
    "full_params = get_full_params(best_state)\n",
    " \n",
    "# with open('finetuned_params.pickle', 'wb') as f:\n",
    "#     pickle.dump(full_params, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning long-range parameters\n",
    "\n",
    "One can also fine-tune the long-range parameters like this form:\n",
    "```python\n",
    "import flax.linen as nn\n",
    "import jax.numpy as jnp\n",
    "\n",
    "class OptPQEqParameters(nn.Module):\n",
    "    init_radius: float\n",
    "    init_chi: float\n",
    "    init_eta: float\n",
    "    init_ks: float\n",
    "    radius_range: Tuple[float, float]\n",
    "    chi_range: Tuple[float, float]\n",
    "    eta_range: Tuple[float, float]\n",
    "    ks_range: Tuple[float, float]\n",
    "    def setup(self):\n",
    "        self.radius_unconstrained = self.param('radius_unconstrained', \n",
    "                                            lambda _: self._inverse_sigmoid_transform(\n",
    "                                                self.init_radius, self.radius_range[0], self.radius_range[1]\n",
    "                                            ))\n",
    "        self.chi_unconstrained = self.param('chi_unconstrained',\n",
    "                                            lambda _: self._inverse_sigmoid_transform(\n",
    "                                                self.init_chi, self.chi_range[0], self.chi_range[1]\n",
    "                                            ))\n",
    "        self.eta_unconstrained = self.param('eta_unconstrained',\n",
    "                                            lambda _: self._inverse_sigmoid_transform(\n",
    "                                                self.init_eta, self.eta_range[0], self.eta_range[1]\n",
    "                                            ))\n",
    "        self.ks_unconstrained = self.param('ks_unconstrained',\n",
    "                                            lambda _: self._inverse_sigmoid_transform(\n",
    "                                                self.init_ks, self.ks_range[0], self.ks_range[1]\n",
    "                                            ))\n",
    "\n",
    "        def _sigmoid_transform(self, x, min_val, max_val):\n",
    "        return min_val + (max_val - min_val) * jax.nn.sigmoid(x)\n",
    "    \n",
    "    def _inverse_sigmoid_transform(self, y, min_val, max_val):\n",
    "        y_norm = (y - min_val) / (max_val - min_val)\n",
    "        y_norm = jnp.clip(y_norm, 1e-6, 1-1e-6)\n",
    "        return jnp.log(y_norm / (1 - y_norm))\n",
    "\n",
    "    def get_constrained_parameters(self):\n",
    "        radius = self._sigmoid_transform(self.radius_unconstrained, \n",
    "                                        self.radius_range[0], self.radius_range[1])\n",
    "        chi = self._sigmoid_transform(self.chi_unconstrained, \n",
    "                                    self.chi_range[0], self.chi_range[1])\n",
    "        eta = self._sigmoid_transform(self.eta_unconstrained, \n",
    "                                    self.eta_range[0], self.eta_range[1])\n",
    "        ks = self._sigmoid_transform(self.ks_unconstrained, \n",
    "                                   self.ks_range[0], self.ks_range[1])\n",
    "        return radius, chi, eta, ks\n",
    "    \n",
    "    def calculate_pqeq(self, radius, chi, eta, ks, **args_for_calculate_pqeq):\n",
    "        # This function should be implemented to calculate the pqeq based on the parameters\n",
    "        # and the input arguments. The implementation will depend on the specific use case.\n",
    "        pass\n",
    "    \n",
    "    @nn.compact\n",
    "    def __call__(self, **args_for_calculate_pqeq):\n",
    "        radius, chi, eta, ks = self.get_constrained_parameters()\n",
    "        return self.calculate_pqeq(radius, chi, eta, ks, **args_for_calculate_pqeq)\n",
    "        # or return value and gradients\n",
    "```\n",
    "\n",
    "Or fine-tune the whole model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
